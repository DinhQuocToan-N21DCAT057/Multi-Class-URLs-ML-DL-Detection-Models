import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import plot_model, to_categorical
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score, precision_recall_curve, auc, confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns


df = pd.read_csv('/content/drive/MyDrive/ISCX-URL-2016/All.csv')
df.head(10)

	Querylength	domain_token_count	path_token_count	avgdomaintokenlen	longdomaintokenlen	avgpathtokenlen	tld	charcompvowels	charcompace	ldl_url	...	SymbolCount_FileName	SymbolCount_Extension	SymbolCount_Afterpath	Entropy_URL	Entropy_Domain	Entropy_DirectoryName	Entropy_Filename	Entropy_Extension	Entropy_Afterpath	URL_Type_obf_Type
0	0	4	5	5.5	14	4.400000	4	8	3	0	...	1	0	-1	0.726298	0.784493	0.894886	0.850608	NaN	-1.0	Defacement
1	0	4	5	5.5	14	6.000000	4	12	4	0	...	0	0	-1	0.688635	0.784493	0.814725	0.859793	0.0	-1.0	Defacement
2	0	4	5	5.5	14	5.800000	4	12	5	0	...	0	0	-1	0.695049	0.784493	0.814725	0.801880	0.0	-1.0	Defacement
3	0	4	12	5.5	14	5.500000	4	32	16	0	...	0	0	-1	0.640130	0.784493	0.814725	0.663210	0.0	-1.0	Defacement
4	0	4	6	5.5	14	7.333334	4	18	11	0	...	0	0	-1	0.681307	0.784493	0.814725	0.804526	0.0	-1.0	Defacement
5	0	4	8	5.5	14	6.500000	4	22	10	0	...	0	0	-1	0.666676	0.784493	0.814725	0.755658	0.0	-1.0	Defacement
6	0	4	5	5.5	14	7.800000	4	17	10	0	...	0	0	-1	0.682440	0.784493	0.814725	0.766719	0.0	-1.0	Defacement
7	0	4	7	5.5	14	6.285714	4	16	9	0	...	0	0	-1	0.709396	0.784493	0.814725	0.797498	0.0	-1.0	Defacement
8	0	4	6	5.5	14	6.500000	4	16	10	0	...	0	0	-1	0.678242	0.784493	0.814725	0.732258	0.0	-1.0	Defacement
9	0	4	5	5.5	14	3.600000	4	7	3	0	...	1	0	-1	0.740950	0.784493	0.894886	0.894886	NaN	-1.0	Defacement
10 rows × 80 columns


# Kiểm tra các giá trị duy nhất
print("Các giá trị duy nhất trong URL_Type_obf_Type:")
print(df['URL_Type_obf_Type'].value_counts())

# Trích xuất chuỗi từ danh sách lồng nhau (từ [[Defacement]] -> [Defacement] -> 'Defacement')
df['URL_Type_obf_Type'] = df['URL_Type_obf_Type'].apply(lambda x: x[0][0] if isinstance(x, list) and len(x) > 0 and isinstance(x[0], list) else x[0] if isinstance(x, list) else x)

# Kiểm tra lại các giá trị duy nhất sau khi trích xuất
print("\nCác giá trị duy nhất sau khi trích xuất:")
print(df['URL_Type_obf_Type'].value_counts())

# Mã hóa nhãn thành số nguyên
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(df['URL_Type_obf_Type'])

# Chuyển thành one-hot encoding
labels_one_hot = to_categorical(labels_encoded)

# Tạo DataFrame cho nhãn one-hot
labels_df = pd.DataFrame(labels_one_hot, columns=label_encoder.classes_)

# Hiển thị thông tin
print("\nCác lớp nhãn:", label_encoder.classes_)
print("\nNhãn one-hot (5 dòng đầu tiên):")
print(labels_df.head())

Các giá trị duy nhất trong URL_Type_obf_Type:
URL_Type_obf_Type
Defacement    7930
benign        7781
phishing      7586
malware       6712
spam          6698
Name: count, dtype: int64

Các giá trị duy nhất sau khi trích xuất:
URL_Type_obf_Type
Defacement    7930
benign        7781
phishing      7586
malware       6712
spam          6698
Name: count, dtype: int64

Các lớp nhãn: ['Defacement' 'benign' 'malware' 'phishing' 'spam']

Nhãn one-hot (5 dòng đầu tiên):
   Defacement  benign  malware  phishing  spam
0         1.0     0.0      0.0       0.0   0.0
1         1.0     0.0      0.0       0.0   0.0
2         1.0     0.0      0.0       0.0   0.0
3         1.0     0.0      0.0       0.0   0.0
4         1.0     0.0      0.0       0.0   0.0


print("Số giá trị NaN trước khi thay thế:")
print(df.isna().sum())

Số giá trị NaN trước khi thay thế:
Querylength                 0
domain_token_count          0
path_token_count            0
avgdomaintokenlen           0
longdomaintokenlen          0
                         ... 
Entropy_DirectoryName    8468
Entropy_Filename          236
Entropy_Extension          40
Entropy_Afterpath           6
URL_Type_obf_Type           0
Length: 80, dtype: int64


# Thay thế tất cả giá trị NaN bằng 0
df_cleaned = df.fillna(0)
print("Số giá trị NaN sau khi thay thế:")
print(df_cleaned.isna().sum())

Số giá trị NaN sau khi thay thế:
Querylength              0
domain_token_count       0
path_token_count         0
avgdomaintokenlen        0
longdomaintokenlen       0
                        ..
Entropy_DirectoryName    0
Entropy_Filename         0
Entropy_Extension        0
Entropy_Afterpath        0
URL_Type_obf_Type        0
Length: 80, dtype: int64


print("Số giá trị -1 trước khi thay thế:")
print((df == -1).sum())

Số giá trị -1 trước khi thay thế:
Querylength                  0
domain_token_count           0
path_token_count             0
avgdomaintokenlen            0
longdomaintokenlen           0
                         ...  
Entropy_DirectoryName     1852
Entropy_Filename          1852
Entropy_Extension         1852
Entropy_Afterpath        20481
URL_Type_obf_Type            0
Length: 80, dtype: int64


# Thay thế tất cả giá trị -1 bằng 0
df_cleaned = df_cleaned.replace(-1, 0)
print("Số giá trị -1 sau khi thay thế:")
print((df_cleaned == -1).sum())

Số giá trị -1 sau khi thay thế:
Querylength              0
domain_token_count       0
path_token_count         0
avgdomaintokenlen        0
longdomaintokenlen       0
                        ..
Entropy_DirectoryName    0
Entropy_Filename         0
Entropy_Extension        0
Entropy_Afterpath        0
URL_Type_obf_Type        0
Length: 80, dtype: int64


# Thay thế tất cả giá trị Inf bằng NaN, sau đó thay bằng trung bình
numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64']).columns
df_numeric = df_cleaned[numeric_columns].values
df_numeric_cleaned = np.where(np.isinf(df_numeric), np.nan, df_numeric)
mean_val = np.nanmean(df_numeric_cleaned)
df_numeric_cleaned = np.nan_to_num(df_numeric_cleaned, nan=mean_val)


# Cập nhật lại DataFrame với dữ liệu đã làm sạch (chỉ cho cột số)
df_cleaned[numeric_columns] = df_numeric_cleaned
print("Số giá trị Inf sau khi thay thế:")
print(np.isinf(df_numeric_cleaned).sum())  # Kiểm tra Inf còn lại

Số giá trị Inf sau khi thay thế:
0


X1 = df_cleaned.drop(columns='URL_Type_obf_Type')
X1.head(10)

	Querylength	domain_token_count	path_token_count	avgdomaintokenlen	longdomaintokenlen	avgpathtokenlen	tld	charcompvowels	charcompace	ldl_url	...	SymbolCount_Directoryname	SymbolCount_FileName	SymbolCount_Extension	SymbolCount_Afterpath	Entropy_URL	Entropy_Domain	Entropy_DirectoryName	Entropy_Filename	Entropy_Extension	Entropy_Afterpath
0	0.0	4.0	5.0	5.5	14.0	4.400000	4.0	8.0	3.0	0.0	...	2.0	1.0	0.0	0.0	0.726298	0.784493	0.894886	0.850608	0.0	0.0
1	0.0	4.0	5.0	5.5	14.0	6.000000	4.0	12.0	4.0	0.0	...	3.0	0.0	0.0	0.0	0.688635	0.784493	0.814725	0.859793	0.0	0.0
2	0.0	4.0	5.0	5.5	14.0	5.800000	4.0	12.0	5.0	0.0	...	3.0	0.0	0.0	0.0	0.695049	0.784493	0.814725	0.801880	0.0	0.0
3	0.0	4.0	12.0	5.5	14.0	5.500000	4.0	32.0	16.0	0.0	...	3.0	0.0	0.0	0.0	0.640130	0.784493	0.814725	0.663210	0.0	0.0
4	0.0	4.0	6.0	5.5	14.0	7.333334	4.0	18.0	11.0	0.0	...	3.0	0.0	0.0	0.0	0.681307	0.784493	0.814725	0.804526	0.0	0.0
5	0.0	4.0	8.0	5.5	14.0	6.500000	4.0	22.0	10.0	0.0	...	3.0	0.0	0.0	0.0	0.666676	0.784493	0.814725	0.755658	0.0	0.0
6	0.0	4.0	5.0	5.5	14.0	7.800000	4.0	17.0	10.0	0.0	...	3.0	0.0	0.0	0.0	0.682440	0.784493	0.814725	0.766719	0.0	0.0
7	0.0	4.0	7.0	5.5	14.0	6.285714	4.0	16.0	9.0	0.0	...	3.0	0.0	0.0	0.0	0.709396	0.784493	0.814725	0.797498	0.0	0.0
8	0.0	4.0	6.0	5.5	14.0	6.500000	4.0	16.0	10.0	0.0	...	3.0	0.0	0.0	0.0	0.678242	0.784493	0.814725	0.732258	0.0	0.0
9	0.0	4.0	5.0	5.5	14.0	3.600000	4.0	7.0	3.0	0.0	...	2.0	1.0	0.0	0.0	0.740950	0.784493	0.894886	0.894886	0.0	0.0
10 rows × 79 columns

Y1 = labels_df
Y1.head(2)

Defacement	benign	malware	phishing	spam
0	1.0	0.0	0.0	0.0	0.0
1	1.0	0.0	0.0	0.0	0.0


Y1.shape
(36707, 5)


from sklearn.model_selection import train_test_split
X_train1,X_test1,Y_train1,Y_test1 = train_test_split(X1,Y1,stratify = Y1,test_size = 0.2,random_state = 42)


X_train1 = np.expand_dims(X_train1, axis=-1)
X_test1 = np.expand_dims(X_test1, axis=-1)

print(X_train1.shape, Y_train1.shape)

(29365, 79, 1) (29365, 5)


input_size1 = X_train1[1].shape


def eval_graph(results):
    # Vẽ đồ thị Binary Accuracy
    binary_acc = results.history['binary_accuracy']
    val_binary_acc = results.history['val_binary_accuracy']
    epochs = range(len(binary_acc))
    fig = plt.figure(figsize=(14, 7))
    plt.plot(epochs, binary_acc, 'r', label="Training Binary Accuracy")
    plt.plot(epochs, val_binary_acc, 'b', label="Validation Binary Accuracy")
    plt.legend(loc='upper left')
    plt.title("BINARY ACCURACY GRAPH")
    plt.show()

    # Vẽ đồ thị Loss
    loss = results.history['loss']
    val_loss = results.history['val_loss']
    epochs = range(len(loss))
    fig = plt.figure(figsize=(14, 7))
    plt.plot(epochs, loss, 'r', label="Training Loss")
    plt.plot(epochs, val_loss, 'b', label="Validation Loss")
    plt.legend(loc='upper left')
    plt.title("LOSS GRAPH")
    plt.show()
	
	
def conf_matrix_multi_label(X_test, Y_test, model, label_names):
    # Đảm bảo X_test và Y_test là mảng NumPy
    X_test = np.array(X_test)
    Y_test = np.array(Y_test)
    if len(X_test.shape) == 2:
        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    Y_pred_proba = model.predict(X_test)
    Y_pred = (Y_pred_proba > 0.5).astype(int)
    
    # Kiểm tra shape
    print("Shape of Y_test:", Y_test.shape)
    print("Shape of Y_pred:", Y_pred.shape)
    
    n_classes = Y_test.shape[1]
    fig, axes = plt.subplots(nrows=1, ncols=n_classes, figsize=(20, 5), squeeze=False)
    fig.suptitle('CONFUSION MATRICES FOR MULTI-LABEL CLASSIFICATION', fontsize=16)
    for i in range(n_classes):
        cm = confusion_matrix(Y_test[:, i], Y_pred[:, i])
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
        disp.plot(ax=axes[0, i], cmap=plt.cm.YlGn, values_format='.0f')
        axes[0, i].set_title(f'{label_names[i]}')
        axes[0, i].set_xlabel('Predicted')
        axes[0, i].set_ylabel('True')
    plt.tight_layout()
    plt.show()
    macro_f1 = f1_score(Y_test, Y_pred, average='macro')
    print(f"Macro F1-Score: {macro_f1:.4f}")
	
	
def pr_auc_multi_label(X_test, Y_test, model, label_names):
    # Đảm bảo X_test và Y_test là mảng NumPy
    X_test = np.array(X_test)
    Y_test = np.array(Y_test)
    if len(X_test.shape) == 2:
        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    Y_pred_proba = model.predict(X_test)
    Y_test_binary = (Y_test > 0.5).astype(int)
    
    n_classes = Y_test.shape[1]
    plt.figure(figsize=(14, 7))
    for i in range(n_classes):
        precision, recall, _ = precision_recall_curve(Y_test_binary[:, i], Y_pred_proba[:, i])
        pr_auc = auc(recall, precision)
        plt.plot(recall, precision, label=f'{label_names[i]} (PR-AUC = {pr_auc:.2f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('PRECISION-RECALL CURVES FOR MULTI-LABEL CLASSIFICATION')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()
    pr_aucs = [auc(precision_recall_curve(Y_test_binary[:, i], Y_pred_proba[:, i])[1],
                   precision_recall_curve(Y_test_binary[:, i], Y_pred_proba[:, i])[0]) for i in range(n_classes)]
    macro_pr_auc = np.mean(pr_aucs)
    print(f"Macro PR-AUC: {macro_pr_auc:.4f}")
	
	
from keras import Sequential
from tensorflow.keras.optimizers import Adam

def CNN(input_size, num_classes=5):
    model = Sequential()
    model.add(layers.Input(input_size))
    model.add(layers.Conv1D(filters=16, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2, padding='same'))
    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2, padding='same'))
    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2, padding='same'))
    model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2, padding='same'))
    model.add(layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2, padding='same'))
    model.add(layers.Flatten())
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(num_classes, activation='sigmoid'))  # Đầu ra cho 5 nhãn

    return model

CNN_model1 = CNN(input_size1)
CNN_model1.summary()
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ conv1d (Conv1D)                 │ (None, 79, 16)         │            64 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (Dropout)               │ (None, 79, 16)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization             │ (None, 79, 16)         │            64 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d (MaxPooling1D)    │ (None, 40, 16)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_1 (Conv1D)               │ (None, 40, 32)         │         1,568 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_1 (Dropout)             │ (None, 40, 32)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_1           │ (None, 40, 32)         │           128 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d_1 (MaxPooling1D)  │ (None, 20, 32)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_2 (Conv1D)               │ (None, 20, 64)         │         6,208 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_2 (Dropout)             │ (None, 20, 64)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_2           │ (None, 20, 64)         │           256 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d_2 (MaxPooling1D)  │ (None, 10, 64)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_3 (Conv1D)               │ (None, 10, 128)        │        24,704 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_3 (Dropout)             │ (None, 10, 128)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_3           │ (None, 10, 128)        │           512 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d_3 (MaxPooling1D)  │ (None, 5, 128)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_4 (Conv1D)               │ (None, 5, 256)         │        98,560 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_4 (Dropout)             │ (None, 5, 256)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_4           │ (None, 5, 256)         │         1,024 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d_4 (MaxPooling1D)  │ (None, 3, 256)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten (Flatten)               │ (None, 768)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_5 (Dropout)             │ (None, 768)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 512)            │       393,728 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_6 (Dropout)             │ (None, 512)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 5)              │         2,565 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 529,381 (2.02 MB)
 Trainable params: 528,389 (2.02 MB)
 Non-trainable params: 992 (3.88 KB)
 
 
# Biên dịch mô hình với learning rate thấp hơn
optimizer = Adam(learning_rate=0.0001)
CNN_model1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])

# Kiểm tra dữ liệu
print("Kiểm tra NaN/Inf trong X_train1:", np.any(np.isnan(X_train1)), np.any(np.isinf(X_train1)))
print("Kiểm tra NaN/Inf trong Y_train1:", np.any(np.isnan(Y_train1)), np.any(np.isinf(Y_train1)))
print("Shape of X_train1:", X_train1.shape)  # (29365, 79, 1)
print("Shape of Y_train1:", Y_train1.shape)  # (29365, 5)
print("Sample of Y_train1:", Y_train1[:5])   # Kiểm tra one-hot
print("Min and Max of X_train1:", np.min(X_train1), np.max(X_train1))  # Kiểm tra phạm vi giá trị

# Kiểm tra phân bố nhãn
y_labels = np.argmax(Y_train1, axis=1)
print("Phân bố nhãn (tỷ lệ):", pd.Series(y_labels).value_counts(normalize=True))

# Định nghĩa callbacks
callbacks = [
    tf.keras.callbacks.ModelCheckpoint('CNN_MODEL_ON_FEATURE_EXTRACTED.h5', verbose=1, save_best_only=True),
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=30, verbose=1)
]

# Huấn luyện mô hình
CNN_results_1 = CNN_model1.fit(X_train1, Y_train1, validation_split=0.2, batch_size=128, epochs=200, callbacks=callbacks)

# Kiểm tra đầu ra mẫu
sample_pred = CNN_model1.predict(X_train1[:10])
print("Sample predictions:", sample_pred)
if np.any(np.isnan(sample_pred)):
    print("Predictions contain NaN!")
	
	
Kiểm tra NaN/Inf trong X_train1: False False
Kiểm tra NaN/Inf trong Y_train1: False False
Shape of X_train1: (29365, 79, 1)
Shape of Y_train1: (29365, 5)
Sample of Y_train1:        Defacement  benign  malware  phishing  spam
26485         0.0     0.0      0.0       1.0   0.0
670           1.0     0.0      0.0       0.0   0.0
11205         0.0     1.0      0.0       0.0   0.0
22872         0.0     0.0      0.0       1.0   0.0
5449          1.0     0.0      0.0       0.0   0.0
Min and Max of X_train1: 0.0 1420.0
Phân bố nhãn (tỷ lệ): 0    0.216040
1    0.211987
3    0.206675
2    0.182837
4    0.182462
Name: proportion, dtype: float64
Epoch 1/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - binary_accuracy: 0.7096 - loss: 0.6542
Epoch 1: val_loss improved from inf to 0.50760, saving model to CNN_MODEL_ON_FEATURE_EXTRACTED.h5
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
184/184 ━━━━━━━━━━━━━━━━━━━━ 24s 93ms/step - binary_accuracy: 0.7101 - loss: 0.6532 - val_binary_accuracy: 0.8089 - val_loss: 0.5076
Epoch 2/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.7912 - loss: 0.4921
Epoch 2: val_loss improved from 0.50760 to 0.41435, saving model to CNN_MODEL_ON_FEATURE_EXTRACTED.h5
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 90ms/step - binary_accuracy: 0.7913 - loss: 0.4919 - val_binary_accuracy: 0.8237 - val_loss: 0.4144
Epoch 3/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8057 - loss: 0.4538
Epoch 3: val_loss improved from 0.41435 to 0.40311, saving model to CNN_MODEL_ON_FEATURE_EXTRACTED.h5
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 87ms/step - binary_accuracy: 0.8058 - loss: 0.4537 - val_binary_accuracy: 0.8261 - val_loss: 0.4031
Epoch 4/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 93ms/step - binary_accuracy: 0.8161 - loss: 0.4251
Epoch 4: val_loss improved from 0.40311 to 0.39617, saving model to CNN_MODEL_ON_FEATURE_EXTRACTED.h5
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
184/184 ━━━━━━━━━━━━━━━━━━━━ 22s 97ms/step - binary_accuracy: 0.8161 - loss: 0.4251 - val_binary_accuracy: 0.8236 - val_loss: 0.3962
Epoch 5/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8220 - loss: 0.4098
Epoch 5: val_loss did not improve from 0.39617
184/184 ━━━━━━━━━━━━━━━━━━━━ 19s 89ms/step - binary_accuracy: 0.8220 - loss: 0.4098 - val_binary_accuracy: 0.8222 - val_loss: 0.3982
Epoch 6/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 121ms/step - binary_accuracy: 0.8284 - loss: 0.3948
Epoch 6: val_loss improved from 0.39617 to 0.39087, saving model to CNN_MODEL_ON_FEATURE_EXTRACTED.h5
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
184/184 ━━━━━━━━━━━━━━━━━━━━ 24s 129ms/step - binary_accuracy: 0.8284 - loss: 0.3948 - val_binary_accuracy: 0.8260 - val_loss: 0.3909
Epoch 7/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8346 - loss: 0.3854
Epoch 7: val_loss improved from 0.39087 to 0.38318, saving model to CNN_MODEL_ON_FEATURE_EXTRACTED.h5
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 86ms/step - binary_accuracy: 0.8346 - loss: 0.3854 - val_binary_accuracy: 0.8278 - val_loss: 0.3832
Epoch 8/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8418 - loss: 0.3681
Epoch 8: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 87ms/step - binary_accuracy: 0.8417 - loss: 0.3682 - val_binary_accuracy: 0.8108 - val_loss: 0.3995
Epoch 9/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8424 - loss: 0.3677
Epoch 9: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 86ms/step - binary_accuracy: 0.8424 - loss: 0.3676 - val_binary_accuracy: 0.8079 - val_loss: 0.4103
Epoch 10/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8462 - loss: 0.3599
Epoch 10: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 86ms/step - binary_accuracy: 0.8462 - loss: 0.3599 - val_binary_accuracy: 0.8059 - val_loss: 0.4014
Epoch 11/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - binary_accuracy: 0.8481 - loss: 0.3533
Epoch 11: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.8481 - loss: 0.3533 - val_binary_accuracy: 0.8074 - val_loss: 0.4008
Epoch 12/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - binary_accuracy: 0.8509 - loss: 0.3499
Epoch 12: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 24s 105ms/step - binary_accuracy: 0.8509 - loss: 0.3499 - val_binary_accuracy: 0.8098 - val_loss: 0.3880
Epoch 13/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8537 - loss: 0.3405
Epoch 13: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.8537 - loss: 0.3405 - val_binary_accuracy: 0.8096 - val_loss: 0.3965
Epoch 14/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - binary_accuracy: 0.8583 - loss: 0.3315
Epoch 14: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 22s 93ms/step - binary_accuracy: 0.8583 - loss: 0.3316 - val_binary_accuracy: 0.8162 - val_loss: 0.4020
Epoch 15/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8592 - loss: 0.3318
Epoch 15: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.8592 - loss: 0.3318 - val_binary_accuracy: 0.8196 - val_loss: 0.3961
Epoch 16/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8648 - loss: 0.3223
Epoch 16: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 17s 90ms/step - binary_accuracy: 0.8648 - loss: 0.3223 - val_binary_accuracy: 0.8243 - val_loss: 0.3905
Epoch 17/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - binary_accuracy: 0.8651 - loss: 0.3202
Epoch 17: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 88ms/step - binary_accuracy: 0.8651 - loss: 0.3201 - val_binary_accuracy: 0.8238 - val_loss: 0.3921
Epoch 18/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - binary_accuracy: 0.8686 - loss: 0.3117
Epoch 18: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 17s 92ms/step - binary_accuracy: 0.8686 - loss: 0.3117 - val_binary_accuracy: 0.8170 - val_loss: 0.4146
Epoch 19/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - binary_accuracy: 0.8690 - loss: 0.3111
Epoch 19: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 93ms/step - binary_accuracy: 0.8690 - loss: 0.3111 - val_binary_accuracy: 0.8225 - val_loss: 0.4007
Epoch 20/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8679 - loss: 0.3138
Epoch 20: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 86ms/step - binary_accuracy: 0.8679 - loss: 0.3137 - val_binary_accuracy: 0.8195 - val_loss: 0.4104
Epoch 21/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8741 - loss: 0.3000
Epoch 21: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 86ms/step - binary_accuracy: 0.8741 - loss: 0.3000 - val_binary_accuracy: 0.8235 - val_loss: 0.3991
Epoch 22/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8749 - loss: 0.2976
Epoch 22: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.8749 - loss: 0.2976 - val_binary_accuracy: 0.8249 - val_loss: 0.4061
Epoch 23/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8765 - loss: 0.2937
Epoch 23: val_loss did not improve from 0.38318
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 86ms/step - binary_accuracy: 0.8766 - loss: 0.2937 - val_binary_accuracy: 0.8278 - val_loss: 0.3915
Epoch 24/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - binary_accuracy: 0.8788 - loss: 0.2907
Epoch 24: val_loss improved from 0.38318 to 0.38168, saving model to CNN_MODEL_ON_FEATURE_EXTRACTED.h5
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
184/184 ━━━━━━━━━━━━━━━━━━━━ 22s 92ms/step - binary_accuracy: 0.8788 - loss: 0.2907 - val_binary_accuracy: 0.8315 - val_loss: 0.3817
Epoch 25/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - binary_accuracy: 0.8789 - loss: 0.2857
Epoch 25: val_loss did not improve from 0.38168
184/184 ━━━━━━━━━━━━━━━━━━━━ 19s 85ms/step - binary_accuracy: 0.8789 - loss: 0.2857 - val_binary_accuracy: 0.8320 - val_loss: 0.3890
Epoch 26/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - binary_accuracy: 0.8813 - loss: 0.2862
Epoch 26: val_loss did not improve from 0.38168
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 88ms/step - binary_accuracy: 0.8813 - loss: 0.2861 - val_binary_accuracy: 0.8266 - val_loss: 0.4078
Epoch 27/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8844 - loss: 0.2764
Epoch 27: val_loss did not improve from 0.38168
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 86ms/step - binary_accuracy: 0.8844 - loss: 0.2764 - val_binary_accuracy: 0.8302 - val_loss: 0.4002
Epoch 28/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8872 - loss: 0.2731
Epoch 28: val_loss did not improve from 0.38168
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.8872 - loss: 0.2731 - val_binary_accuracy: 0.8343 - val_loss: 0.3902
Epoch 29/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8859 - loss: 0.2708
Epoch 29: val_loss did not improve from 0.38168
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 86ms/step - binary_accuracy: 0.8859 - loss: 0.2708 - val_binary_accuracy: 0.8309 - val_loss: 0.3946
Epoch 30/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - binary_accuracy: 0.8892 - loss: 0.2700
Epoch 30: val_loss did not improve from 0.38168
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 87ms/step - binary_accuracy: 0.8892 - loss: 0.2700 - val_binary_accuracy: 0.8283 - val_loss: 0.4171
Epoch 31/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8907 - loss: 0.2620
Epoch 31: val_loss did not improve from 0.38168
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 89ms/step - binary_accuracy: 0.8907 - loss: 0.2620 - val_binary_accuracy: 0.8290 - val_loss: 0.4124
Epoch 32/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - binary_accuracy: 0.8903 - loss: 0.2629
Epoch 32: val_loss did not improve from 0.38168
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 87ms/step - binary_accuracy: 0.8903 - loss: 0.2628 - val_binary_accuracy: 0.8287 - val_loss: 0.4037
Epoch 33/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8939 - loss: 0.2560
Epoch 33: val_loss improved from 0.38168 to 0.37739, saving model to CNN_MODEL_ON_FEATURE_EXTRACTED.h5
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.8939 - loss: 0.2560 - val_binary_accuracy: 0.8383 - val_loss: 0.3774
Epoch 34/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8931 - loss: 0.2534
Epoch 34: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 89ms/step - binary_accuracy: 0.8931 - loss: 0.2534 - val_binary_accuracy: 0.8359 - val_loss: 0.3856
Epoch 35/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.8972 - loss: 0.2496
Epoch 35: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 86ms/step - binary_accuracy: 0.8972 - loss: 0.2496 - val_binary_accuracy: 0.8313 - val_loss: 0.4011
Epoch 36/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8965 - loss: 0.2471
Epoch 36: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 87ms/step - binary_accuracy: 0.8965 - loss: 0.2471 - val_binary_accuracy: 0.8274 - val_loss: 0.4145
Epoch 37/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.8968 - loss: 0.2454
Epoch 37: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 17s 90ms/step - binary_accuracy: 0.8968 - loss: 0.2454 - val_binary_accuracy: 0.8278 - val_loss: 0.4135
Epoch 38/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.9004 - loss: 0.2423
Epoch 38: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 86ms/step - binary_accuracy: 0.9004 - loss: 0.2423 - val_binary_accuracy: 0.8246 - val_loss: 0.4172
Epoch 39/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.9022 - loss: 0.2361
Epoch 39: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.9022 - loss: 0.2361 - val_binary_accuracy: 0.8195 - val_loss: 0.4274
Epoch 40/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9030 - loss: 0.2358
Epoch 40: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 90ms/step - binary_accuracy: 0.9030 - loss: 0.2358 - val_binary_accuracy: 0.8238 - val_loss: 0.4110
Epoch 41/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - binary_accuracy: 0.9028 - loss: 0.2321
Epoch 41: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 93ms/step - binary_accuracy: 0.9028 - loss: 0.2321 - val_binary_accuracy: 0.8241 - val_loss: 0.4120
Epoch 42/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.9032 - loss: 0.2321
Epoch 42: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 86ms/step - binary_accuracy: 0.9032 - loss: 0.2321 - val_binary_accuracy: 0.8185 - val_loss: 0.4362
Epoch 43/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9062 - loss: 0.2276
Epoch 43: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 86ms/step - binary_accuracy: 0.9062 - loss: 0.2276 - val_binary_accuracy: 0.8151 - val_loss: 0.4398
Epoch 44/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - binary_accuracy: 0.9055 - loss: 0.2261
Epoch 44: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 22s 92ms/step - binary_accuracy: 0.9055 - loss: 0.2261 - val_binary_accuracy: 0.8236 - val_loss: 0.4199
Epoch 45/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.9074 - loss: 0.2215
Epoch 45: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 89ms/step - binary_accuracy: 0.9074 - loss: 0.2216 - val_binary_accuracy: 0.8207 - val_loss: 0.4239
Epoch 46/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 102ms/step - binary_accuracy: 0.9104 - loss: 0.2167
Epoch 46: val_loss did not improve from 0.37739
184/184 ━━━━━━━━━━━━━━━━━━━━ 24s 109ms/step - binary_accuracy: 0.9104 - loss: 0.2167 - val_binary_accuracy: 0.8354 - val_loss: 0.3898
Epoch 47/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - binary_accuracy: 0.9095 - loss: 0.2187
Epoch 47: val_loss improved from 0.37739 to 0.37188, saving model to CNN_MODEL_ON_FEATURE_EXTRACTED.h5
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
184/184 ━━━━━━━━━━━━━━━━━━━━ 17s 90ms/step - binary_accuracy: 0.9095 - loss: 0.2187 - val_binary_accuracy: 0.8427 - val_loss: 0.3719
Epoch 48/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9097 - loss: 0.2187
Epoch 48: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 87ms/step - binary_accuracy: 0.9097 - loss: 0.2187 - val_binary_accuracy: 0.8286 - val_loss: 0.4055
Epoch 49/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.9101 - loss: 0.2193
Epoch 49: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 86ms/step - binary_accuracy: 0.9101 - loss: 0.2192 - val_binary_accuracy: 0.8302 - val_loss: 0.4053
Epoch 50/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - binary_accuracy: 0.9128 - loss: 0.2109
Epoch 50: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 22s 92ms/step - binary_accuracy: 0.9128 - loss: 0.2110 - val_binary_accuracy: 0.8223 - val_loss: 0.4268
Epoch 51/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9136 - loss: 0.2117
Epoch 51: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 87ms/step - binary_accuracy: 0.9136 - loss: 0.2117 - val_binary_accuracy: 0.8250 - val_loss: 0.4055
Epoch 52/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9112 - loss: 0.2110
Epoch 52: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.9112 - loss: 0.2110 - val_binary_accuracy: 0.8373 - val_loss: 0.3869
Epoch 53/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - binary_accuracy: 0.9157 - loss: 0.2055
Epoch 53: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 88ms/step - binary_accuracy: 0.9157 - loss: 0.2055 - val_binary_accuracy: 0.8249 - val_loss: 0.4174
Epoch 54/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9155 - loss: 0.2056
Epoch 54: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 90ms/step - binary_accuracy: 0.9155 - loss: 0.2056 - val_binary_accuracy: 0.8335 - val_loss: 0.4030
Epoch 55/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - binary_accuracy: 0.9184 - loss: 0.1995
Epoch 55: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 88ms/step - binary_accuracy: 0.9184 - loss: 0.1995 - val_binary_accuracy: 0.8256 - val_loss: 0.4371
Epoch 56/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - binary_accuracy: 0.9183 - loss: 0.1990
Epoch 56: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 91ms/step - binary_accuracy: 0.9183 - loss: 0.1990 - val_binary_accuracy: 0.8269 - val_loss: 0.4286
Epoch 57/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9184 - loss: 0.1991
Epoch 57: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 17s 90ms/step - binary_accuracy: 0.9184 - loss: 0.1991 - val_binary_accuracy: 0.8311 - val_loss: 0.4132
Epoch 58/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9177 - loss: 0.2004
Epoch 58: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 90ms/step - binary_accuracy: 0.9177 - loss: 0.2004 - val_binary_accuracy: 0.8179 - val_loss: 0.4643
Epoch 59/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - binary_accuracy: 0.9190 - loss: 0.1982
Epoch 59: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 17s 93ms/step - binary_accuracy: 0.9190 - loss: 0.1982 - val_binary_accuracy: 0.8244 - val_loss: 0.4401
Epoch 60/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - binary_accuracy: 0.9190 - loss: 0.1965
Epoch 60: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 89ms/step - binary_accuracy: 0.9190 - loss: 0.1965 - val_binary_accuracy: 0.8361 - val_loss: 0.3950
Epoch 61/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.9205 - loss: 0.1952
Epoch 61: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 86ms/step - binary_accuracy: 0.9205 - loss: 0.1952 - val_binary_accuracy: 0.8332 - val_loss: 0.4104
Epoch 62/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9212 - loss: 0.1930
Epoch 62: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 88ms/step - binary_accuracy: 0.9212 - loss: 0.1930 - val_binary_accuracy: 0.8238 - val_loss: 0.4444
Epoch 63/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9225 - loss: 0.1883
Epoch 63: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 86ms/step - binary_accuracy: 0.9225 - loss: 0.1883 - val_binary_accuracy: 0.8371 - val_loss: 0.3971
Epoch 64/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9219 - loss: 0.1892
Epoch 64: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 88ms/step - binary_accuracy: 0.9219 - loss: 0.1892 - val_binary_accuracy: 0.8255 - val_loss: 0.4445
Epoch 65/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9216 - loss: 0.1922
Epoch 65: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.9216 - loss: 0.1922 - val_binary_accuracy: 0.8232 - val_loss: 0.4516
Epoch 66/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9221 - loss: 0.1887
Epoch 66: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 91ms/step - binary_accuracy: 0.9221 - loss: 0.1887 - val_binary_accuracy: 0.8166 - val_loss: 0.4838
Epoch 67/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - binary_accuracy: 0.9229 - loss: 0.1868
Epoch 67: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 17s 94ms/step - binary_accuracy: 0.9229 - loss: 0.1868 - val_binary_accuracy: 0.8386 - val_loss: 0.3957
Epoch 68/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9253 - loss: 0.1845
Epoch 68: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 88ms/step - binary_accuracy: 0.9253 - loss: 0.1845 - val_binary_accuracy: 0.8367 - val_loss: 0.4129
Epoch 69/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.9247 - loss: 0.1844
Epoch 69: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 87ms/step - binary_accuracy: 0.9247 - loss: 0.1844 - val_binary_accuracy: 0.8192 - val_loss: 0.4801
Epoch 70/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9253 - loss: 0.1848
Epoch 70: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 87ms/step - binary_accuracy: 0.9253 - loss: 0.1847 - val_binary_accuracy: 0.8357 - val_loss: 0.4033
Epoch 71/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9268 - loss: 0.1814
Epoch 71: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.9268 - loss: 0.1814 - val_binary_accuracy: 0.8393 - val_loss: 0.3898
Epoch 72/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9259 - loss: 0.1808
Epoch 72: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 87ms/step - binary_accuracy: 0.9259 - loss: 0.1808 - val_binary_accuracy: 0.8204 - val_loss: 0.4730
Epoch 73/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9269 - loss: 0.1783
Epoch 73: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 16s 87ms/step - binary_accuracy: 0.9269 - loss: 0.1783 - val_binary_accuracy: 0.8269 - val_loss: 0.4353
Epoch 74/200
184/184 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - binary_accuracy: 0.9282 - loss: 0.1778
Epoch 74: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 17s 93ms/step - binary_accuracy: 0.9282 - loss: 0.1779 - val_binary_accuracy: 0.8269 - val_loss: 0.4432
Epoch 75/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - binary_accuracy: 0.9271 - loss: 0.1772
Epoch 75: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 89ms/step - binary_accuracy: 0.9271 - loss: 0.1772 - val_binary_accuracy: 0.8418 - val_loss: 0.3994
Epoch 76/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - binary_accuracy: 0.9278 - loss: 0.1771
Epoch 76: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 21s 91ms/step - binary_accuracy: 0.9278 - loss: 0.1771 - val_binary_accuracy: 0.8414 - val_loss: 0.3932
Epoch 77/200
183/184 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - binary_accuracy: 0.9278 - loss: 0.1772
Epoch 77: val_loss did not improve from 0.37188
184/184 ━━━━━━━━━━━━━━━━━━━━ 20s 87ms/step - binary_accuracy: 0.9278 - loss: 0.1771 - val_binary_accuracy: 0.8462 - val_loss: 0.3794
Epoch 77: early stopping
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 324ms/step
Sample predictions: [[1.14143461e-01 4.98261720e-01 1.17314135e-04 2.93230880e-02
  1.27762258e-01]
 [7.50797868e-01 7.96169392e-04 3.89453635e-05 4.89979889e-03
  3.91402930e-01]
 [1.02342786e-02 9.78474438e-01 4.11177927e-04 2.74411961e-03
  3.28174763e-04]
 [1.24290120e-03 3.81659865e-02 1.93641022e-01 7.94408143e-01
  3.53631820e-03]
 [1.44892022e-01 7.04960585e-01 1.26333058e-01 1.41459960e-03
  1.21661264e-03]
 [5.14606774e-01 1.51043583e-03 5.51888719e-02 3.06247873e-03
  1.09053127e-01]
 [9.86682832e-01 1.64721587e-05 2.22099405e-02 6.49394860e-06
  1.19365468e-04]
 [9.77099240e-01 4.87052050e-04 8.61599017e-03 2.85260263e-03
  7.76605168e-03]
 [6.90565705e-01 2.96349870e-03 4.50322121e-01 5.60172957e-05
  4.66627243e-04]
 [7.91055057e-03 7.67389126e-03 5.85473359e-01 2.39976309e-03
  2.41856650e-01]]
  
  
CNN_model1.evaluate(X_test1,Y_test1,verbose = 1)

230/230 ━━━━━━━━━━━━━━━━━━━━ 2s 7ms/step - binary_accuracy: 0.8550 - loss: 0.3704
[0.3762913644313812, 0.8520565032958984]

