# -*- coding: utf-8 -*-
"""Multi-Labels-URLs-ML&DL-Detection-Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jWL0boU2t6snL9W7IiZssL720uJQvA67
"""

!pip install pandas numpy matplotlib seaborn scikit-learn tensorflow keras torch nltk joblib pydot torchviz torchinfo xgboost

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import re
import seaborn as sns
import joblib

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.metrics import f1_score, precision_recall_curve, auc, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.utils import resample
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras import Sequential
from tensorflow.keras.utils import plot_model, to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance

df = pd.read_csv('/content/drive/MyDrive/TTTN-Dataset/balanced_dataset_1.csv')
df.head(10)

# Preprocess labels
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(df['label'])
labels_one_hot = tf.keras.utils.to_categorical(labels_encoded)
label_names = label_encoder.classes_

X1 = df.drop(columns=['label', 'url'])
X1.shape

# MinMaxScaler except for these columns
columns_to_scale = [col for col in X1.columns if col not in ['domain_reg_len', 'domain_age', 'page_rank', 'google_index']]
scaler = MinMaxScaler(feature_range=(0, 1))
X1[columns_to_scale] = scaler.fit_transform(X1[columns_to_scale])
joblib.dump(scaler, 'scaler.pkl') #Download for predict
X1.head(10)

Y1 = pd.DataFrame(labels_one_hot, columns=label_encoder.classes_)
Y1.to_csv('/content/drive/MyDrive/TTTN-Dataset/labels_one_hot.csv', index=False)
Y3 = np.argmax(Y1.values, axis=1)
Y1.shape

X2 = df['url']
X2.head(2)

"""# CREATE DATASET WITH PRIOR FEATURES EXTRACTION"""

X_train1,X_test1,Y_train1,Y_test1 = train_test_split(X1,Y1,stratify = Y1,test_size = 0.2,random_state = 42)

X_train1 = np.expand_dims(X_train1, axis=-1)
X_test1 = np.expand_dims(X_test1, axis=-1)

print(X_train1.shape, Y_train1.shape)

input_size1 = X_train1[1].shape

"""# CREATE DATASET WITHOUT PRIOR FEATURES EXTRACTION"""

nltk.download('stopwords')

ps = PorterStemmer()
corpus_train = []

def albumentations(X, corpus_t):
    for i in range(len(X)):
        print(i, "/", len(X))
        review = X[i].decode('utf-8') if isinstance(X[i], bytes) else X[i]
        review = re.sub(r'\?.*', '', review)  # Bỏ query parameters
        review = re.sub(r'[^a-zA-Z0-9\-\/.]', ' ', review)
        review = review.lower()
        review = review.split()
        review = [ps.stem(word) for word in review if word not in set(stopwords.words("english"))]
        review = " ".join(review)
        corpus_t.append(review)

albumentations(X2, corpus_train)

corpus_train[:4]

cv = TfidfVectorizer(max_features=300)
X2 = cv.fit_transform(corpus_train)
joblib.dump(cv, "tfidf_vectorizer.pkl") # Download for predict

X_train2,X_test2,Y_train2,Y_test2 = train_test_split(X2,Y1,stratify = Y1,test_size = 0.2,random_state = 42)

X_train2, Y_train2 = resample(X_train2, Y_train2, n_samples=150000, random_state=42, stratify=Y_train2)

X_train2 = np.expand_dims(X_train2.toarray(), axis=-1)  # Shape: (308208, 500, 1)
X_test2 = np.expand_dims(X_test2.toarray(), axis=-1)    # Shape: (77052, 500, 1)
print(X_train2.shape,Y_train2.shape)
print(X_test2.shape,Y_test2.shape)

# train_dataset = tf.data.Dataset.from_tensor_slices((X_train2, Y_train2))
# train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128).prefetch(tf.data.AUTOTUNE)

# val_size = int(0.2 * X_train2.shape[0])
# val_dataset = tf.data.Dataset.from_tensor_slices((X_train2[-val_size:], Y_train2[-val_size:]))
# val_dataset = val_dataset.batch(128).prefetch(tf.data.AUTOTUNE)

# for x_batch, y_batch in train_dataset.take(1):
#     print(x_batch.shape)
#     print(y_batch.shape)

input_size2 = X_train2[2].shape
input_size1,input_size2

"""# CREATE DATASET FOR XGB (NUMERICAL)"""

X3 = df.drop(columns=['label', 'url'])
X_train3,X_test3,Y_train3,Y_test3 = train_test_split(X3,Y3,stratify = Y3,test_size = 0.2,random_state = 42)
print(X_train3.shape,Y_train3.shape)
print(X_test3.shape,Y_test3.shape)

"""# CREATE DATASET FOR XGB (NON-NUMERICAL)"""

X4 = df['url']

nltk.download('stopwords')
ps = PorterStemmer()
stopwords = set(stopwords.words("english"))

corpus_train = []
def albumentations(X, corpus_t):
    for i in range(len(X)):
        print(i, "/", len(X))
        review = X[i].decode('utf-8') if isinstance(X[i], bytes) else str(X[i])  # Ensure string handling
        review = re.sub(r'\?.*', '', review)  # Remove query parameters
        review = re.sub(r'[^a-zA-Z0-9\-\/.]', ' ', review)  # Keep -, /, .
        review = review.lower()
        review = review.split()
        review = [ps.stem(word) for word in review if word not in stopwords and len(word) > 2]  # Use stopwords_set
        review = " ".join(review)
        corpus_t.append(review)

albumentations(X4, corpus_train)

cv = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
X4 = cv.fit_transform(corpus_train)
joblib.dump(cv, "tfidf_vectorizer_XGB.pkl")

X_train4, X_test4, Y_train4, Y_test4 = train_test_split(X4, Y3, stratify=Y3, test_size=0.2, random_state=42)
print(X_train4.shape, Y_train4.shape)
print(X_test4.shape, Y_test4.shape)

"""# EVALUATION HELPER FUNCTIONS"""

def eval_graph_CNN(results):
    plt.figure(figsize=(14, 7))
    plt.plot(results.history['accuracy'], 'r', label="Training Accuracy")
    plt.plot(results.history['val_accuracy'], 'b', label="Validation Accuracy")
    plt.legend(loc='upper left')
    plt.title("ACCURACY GRAPH")
    plt.show()
    plt.figure(figsize=(14, 7))
    plt.plot(results.history['loss'], 'r', label="Training Loss")
    plt.plot(results.history['val_loss'], 'b', label="Validation Loss")
    plt.legend(loc='upper left')
    plt.title("LOSS GRAPH")
    plt.show()

def conf_matrix_multi_label_CNN(X_test, Y_test, model, label_names):
    Y_pred = np.argmax(model.predict(X_test), axis=1)
    Y_test_labels = np.argmax(Y_test, axis=1)
    cm = confusion_matrix(Y_test_labels, Y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)
    disp.plot(cmap=plt.cm.YlGn)
    plt.title('CONFUSION MATRIX')
    plt.show()
    f1 = f1_score(Y_test_labels, Y_pred, average='macro')
    print(f"Macro F1-Score: {f1:.4f}")

def pr_auc_multi_label(X_test, Y_test, model, label_names):
    # Đảm bảo X_test và Y_test là mảng NumPy
    X_test = np.array(X_test)
    Y_test = np.array(Y_test)
    if len(X_test.shape) == 2:
        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

    Y_pred_proba = model.predict(X_test)
    Y_test_binary = (Y_test > 0.5).astype(int)

    n_classes = Y_test.shape[1]
    plt.figure(figsize=(14, 7))
    for i in range(n_classes):
        precision, recall, _ = precision_recall_curve(Y_test_binary[:, i], Y_pred_proba[:, i])
        pr_auc = auc(recall, precision)
        plt.plot(recall, precision, label=f'{label_names[i]} (PR-AUC = {pr_auc:.2f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('PRECISION-RECALL CURVES FOR MULTI-LABEL CLASSIFICATION')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()
    pr_aucs = [auc(precision_recall_curve(Y_test_binary[:, i], Y_pred_proba[:, i])[1],
                   precision_recall_curve(Y_test_binary[:, i], Y_pred_proba[:, i])[0]) for i in range(n_classes)]
    macro_pr_auc = np.mean(pr_aucs)
    print(f"Macro PR-AUC: {macro_pr_auc:.4f}")

def conf_matrix_multi_label_XGB(X_test, y_test, model, label_names):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)
    disp.plot(cmap=plt.cm.YlGn)
    plt.title('CONFUSION MATRIX (XGBoost)')
    plt.show()
    f1 = f1_score(y_test, y_pred, average='macro')
    print(f"Macro F1-Score: {f1:.4f}")

def eval_graph_XGB(results):
    epochs = range(len(results['validation_0']['mlogloss']))
    plt.figure(figsize=(14, 7))
    plt.plot(epochs, [1 - x for x in results['validation_0']['merror']], 'b', label="Validation Accuracy (1 - Error)")
    plt.legend(loc='upper left')
    plt.title("ACCURACY GRAPH (XGBoost)")
    plt.xlabel("Iteration")
    plt.ylabel("Accuracy")
    plt.show()
    plt.figure(figsize=(14, 7))
    plt.plot(epochs, results['validation_0']['mlogloss'], 'b', label="Validation Loss (Log Loss)")
    plt.legend(loc='upper left')
    plt.title("LOSS GRAPH (XGBoost)")
    plt.xlabel("Iteration")
    plt.ylabel("Log Loss")
    plt.show()

def conf_matrix_multi_label_RF(X_test, y_test, model, label_names):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)
    disp.plot(cmap=plt.cm.YlGn)
    plt.title('CONFUSION MATRIX (Random Forest)')
    plt.show()
    f1 = f1_score(y_test, y_pred, average='macro')
    print(f"Macro F1-Score: {f1:.4f}")

def pr_auc_multi_class_RF(X_test, y_test, model, label_names):
    y_pred_proba = model.predict_proba(X_test)  # Shape: (n_samples, n_classes)
    y_test_one_hot = np.eye(len(label_names))[y_test]  # Chuyển y_test sang one-hot
    n_classes = len(label_names)
    plt.figure(figsize=(14, 7))
    pr_aucs = []
    for i in range(n_classes):
        precision, recall, _ = precision_recall_curve(y_test_one_hot[:, i], y_pred_proba[:, i])
        pr_auc = auc(recall, precision)
        pr_aucs.append(pr_auc)
        plt.plot(recall, precision, label=f'{label_names[i]} (PR-AUC = {pr_auc:.2f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('PRECISION-RECALL CURVES FOR MULTI-CLASS CLASSIFICATION (Random Forest)')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()
    macro_pr_auc = np.mean(pr_aucs)
    print(f"Macro PR-AUC: {macro_pr_auc:.4f}")

"""# CNN MODEL(Multi-Labels)"""

def CNN1(input_size, num_classes=4):
    model = keras.Sequential()
    model.add(layers.Input(input_size))
    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.3))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2, padding='same'))
    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.3))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.3))
    model.add(layers.BatchNormalization())
    model.add(layers.Flatten())
    model.add(layers.Dense(64 if input_size[0] <= 100 else 128, activation='relu', kernel_regularizer=l2(0.01)))
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(num_classes, activation='softmax'))

    return model

def CNN2(input_size, num_classes=4):
    model = keras.Sequential()
    model.add(layers.Input(input_size))
    model.add(layers.Conv1D(filters=16, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.3))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2, padding='same'))
    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.3))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2, padding='same'))
    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))
    model.add(layers.Dropout(0.3))
    model.add(layers.BatchNormalization())
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(num_classes, activation='softmax'))

    return model

CNN_model1 = CNN1(input_size1)
CNN_model1.summary()

plot_model(CNN_model1, to_file='CNN_model1.png', show_shapes=True, show_layer_names=True, dpi=96)

CNN_model2 = CNN2(input_size2)
CNN_model2.summary()

plot_model(CNN_model2, to_file='CNN_model2.png', show_shapes=True, show_layer_names=True, dpi=96)

"""# CNN MODEL TRANNING WITH NUMERICAL FEATURES"""

# Biên dịch mô hình với learning rate thấp hơn
CNN_model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])

# Kiểm tra dữ liệu
print("Kiểm tra NaN/Inf trong X_train1:", np.any(np.isnan(X_train1)), np.any(np.isinf(X_train1)))
print("Kiểm tra NaN/Inf trong Y_train1:", np.any(np.isnan(Y_train1)), np.any(np.isinf(Y_train1)))
print("Shape of X_train1:", X_train1.shape)  # (x, 71, 1)
print("Shape of Y_train1:", Y_train1.shape)  # (x, 4)
print("Sample of Y_train1:", Y_train1[:5])   # Kiểm tra one-hot
print("Min and Max of X_train1:", np.min(X_train1), np.max(X_train1))  # Kiểm tra phạm vi giá trị

# Kiểm tra phân bố nhãn
y_labels = np.argmax(Y_train1, axis=1)
print("Phân bố nhãn (tỷ lệ):", pd.Series(y_labels).value_counts(normalize=True))

# Định nghĩa callbacks
callbacks = [
    ModelCheckpoint('CNN_MODEL_ON_NUMERICAL_FEATURES.keras', save_best_only=True, verbose=1),
    EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)
]

# Huấn luyện mô hình
CNN_results_1 = CNN_model1.fit(X_train1, Y_train1, validation_split=0.2, batch_size=64, epochs=100, callbacks=callbacks)

# Kiểm tra đầu ra mẫu
sample_pred = CNN_model1.predict(X_train1[:10])
print("Sample predictions:", sample_pred)
if np.any(np.isnan(sample_pred)):
    print("Predictions contain NaN!")

CNN_model1.evaluate(X_test1,Y_test1,verbose = 1)

eval_graph_CNN(CNN_results_1)

label_names = ['benign', 'defacement', 'malware', 'phishing']
conf_matrix_multi_label_CNN(X_test1, Y_test1, CNN_model1, label_names)

pr_auc_multi_label(X_test1, Y_test1, CNN_model1, label_names)

"""# CNN MODEL TRANNING WITH NON-NUMERICAL FEATURES"""

class_weights = {0: 0.9, 1: 1.0, 2: 1.1, 3: 1.05}

CNN_model2 = CNN2(input_size2)
CNN_model2.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='categorical_crossentropy',
    metrics=['accuracy', 'Precision', 'Recall']
)

# Kiểm tra phân bố nhãn
y_labels = np.argmax(Y_train2, axis=1)
print("Phân bố nhãn (tỷ lệ):", pd.Series(y_labels).value_counts(normalize=True))

# Định nghĩa callbacks
callbacks = [
    ModelCheckpoint(
        'CNN_MODEL_ON_NON_NUMERICAL_FEATURES.weights.h5',
        save_best_only=True,
        save_weights_only=True,
        verbose=1
    ),
    EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)
]

# Huấn luyện mô hình
CNN_results_2 = CNN_model2.fit(
    X_train2,
    Y_train2,
    validation_split=0.2,
    batch_size=128,
    epochs=100,
    class_weight=class_weights,
    callbacks=callbacks
)

# Kiểm tra đầu ra mẫu
sample_pred = CNN_model2.predict(X_train2[:10])
print("Sample predictions:", sample_pred)
if np.any(np.isnan(sample_pred)):
    print("Predictions contain NaN!")

CNN_model2.evaluate(X_test2,Y_test2,verbose = 1)

eval_graph_CNN(CNN_results_2)

label_names = ['benign', 'defacement', 'malware', 'phishing']
conf_matrix_multi_label_CNN(X_test2, Y_test2, CNN_model2, label_names)

pr_auc_multi_label(X_test2, Y_test2, CNN_model2, label_names)

CNN_model2.save('CNN_MODEL_ON_NON_NUMERICAL_FEATURES.keras')

"""# XGB MODEL (Multi-Label)"""

# Kiểm tra phân bố nhãn
print("Phân bố nhãn (tỷ lệ):", pd.Series(Y3).value_counts(normalize=True))

XGB = XGBClassifier(
    objective='multi:softmax',  # Multi-class classification
    num_class=4,                # Number of classes
    learning_rate=0.1,          # Initial learning rate
    n_estimators=100,           # Number of trees
    max_depth=6,                # Maximum depth of trees
    scale_pos_weight=[1/0.274521, 1/0.245943, 1/0.236030, 1/0.243506],  # Adjusted for class imbalance (based on proportions)
    eval_metric=['mlogloss', 'merror'],
    callbacks=[xgb.callback.EvaluationMonitor(period=1)],
    random_state=42
)

"""# XGB TRANNING WITH NUMERICAL FEATURES"""

XGB.fit(
    X_train3,
    Y_train3,
    eval_set=[(X_test3, Y_test3)],
    verbose=True,  # Enable verbose output for monitoring
)

results = XGB.evals_result()
eval_graph_XGB(results)

label_names = ['benign', 'defacement', 'malware', 'phishing']
conf_matrix_multi_label_XGB(X_test3, Y_test3, XGB, label_names)

joblib.dump(XGB, 'XGB_MODEL_ON_NUMERICAL_FEATURES.pkl')

"""# XGB TRANNING WITH NON-NUMERICAL FEATURES"""

XGB.fit(
    X_train4,
    Y_train4,
    eval_set=[(X_test4, Y_test4)],
    verbose=True,  # Enable verbose output for monitoring
)

results = XGB.evals_result()
eval_graph_XGB(results)

label_names = ['benign', 'defacement', 'malware', 'phishing']
conf_matrix_multi_label_XGB(X_test4, Y_test4, XGB, label_names)

joblib.dump(XGB, 'XGB_MODEL_ON_NON_NUMERICAL_FEATURES.pkl')

"""# RANDOM FOREST MODEL (Multi-Labels)"""

# Kiểm tra phân bố nhãn
print("Phân bố nhãn (tỷ lệ):", pd.Series(Y3).value_counts(normalize=True))

RF = RandomForestClassifier(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=1,
    max_features='sqrt',
    bootstrap=True,
    n_jobs=-1,
    random_state=42
)

"""# RANDOM FOREST TRAINNING WITH NUMERICAL FEATURES"""

RF.fit(X_train3, Y_train3)

label_names = ['benign', 'defacement', 'malware', 'phishing']
conf_matrix_multi_label_RF(X_test3, Y_test3, RF, label_names)

pr_auc_multi_class_RF(X_test3, Y_test3, RF, label_names)

joblib.dump(RF, 'RF_MODEL_ON_NUMERICAL_FEATURES.pkl')

"""# RANDOM FOREST TRAINNING WITH NON-NUMERICAL FEATURES"""

RF.fit(X_train4, Y_train4)

label_names = ['benign', 'defacement', 'malware', 'phishing']
conf_matrix_multi_label_RF(X_test4, Y_test4, RF, label_names)

pr_auc_multi_class_RF(X_test4, Y_test4, RF, label_names)

joblib.dump(RF, 'RF_MODEL_ON_NON_NUMERICAL_FEATURES.pkl')